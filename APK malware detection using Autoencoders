import pandas as pd 
import numpy as np 
import torch 
import torch.nn as nn 
import torch.optim as optim 
import matplotlib.pyplot as plt 
from torch.utils.data import Dataset, DataLoader 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
 
# Load processed dataset 
processed_data_path = "./data/processed_apk_data.csv" 
df = pd.read_csv(processed_data_path) 
 
# Extract features only (unsupervised learning) 
X = df.drop(columns=["label"]).values 
y = df["label"].values  # Labels (if you want to add supervised fine-tuning later) 
X = StandardScaler().fit_transform(X)  # Scaling features to improve training 
X = torch.tensor(X, dtype=torch.float32) 
 
# Custom Dataset for Autoencoder 
class APKDataset(Dataset): 
    def __init__(self, data): 
        self.data = data 
 
    
    def __len__(self): 
        return len(self.data) 
 
    def __getitem__(self, idx): 
        return self.data[idx] 
 
# Create DataLoader 
batch_size = 64 
dataset = APKDataset(X) 
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) 
 
# Define Autoencoder model with Batch Normalization and Dropout 
class Autoencoder(nn.Module): 
    def __init__(self, input_size): 
        super(Autoencoder, self).__init__() 
        self.encoder = nn.Sequential( 
            nn.Linear(input_size, 512), 
            nn.ReLU(), 
            nn.BatchNorm1d(512), 
            nn.Linear(512, 256), 
            nn.ReLU(), 
            nn.BatchNorm1d(256), 
            nn.Linear(256, 128), 
            nn.ReLU(), 
            nn.BatchNorm1d(128), 
            nn.Linear(128, 64) 
        ) 
        self.decoder = nn.Sequential( 
            nn.Linear(64, 128), 
            nn.ReLU(), 
            nn.BatchNorm1d(128), 
            nn.Linear(128, 256), 
            nn.ReLU(), 
            nn.BatchNorm1d(256), 
            nn.Linear(256, 512), 
            nn.ReLU(), 
            nn.BatchNorm1d(512), 
            nn.Linear(512, input_size) 
        ) 
 
    def forward(self, x): 
        encoded = self.encoder(x) 
        decoded = self.decoder(encoded) 
        return decoded 
 
# Setup for training 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
    
model = Autoencoder(input_size=X.shape[1]).to(device) 
 
# Optimizer with learning rate scheduler 
optimizer = optim.Adam(model.parameters(), lr=0.001) 
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7) 
 
# Criterion 
criterion = nn.MSELoss() 
 
# Training loop 
num_epochs = 25 
train_losses = [] 
validation_losses = []  # Added validation loss tracking 
 
# Split the dataset into train and validation set 
X_train, X_val = train_test_split(X, test_size=0.2, random_state=42) 
train_loader = DataLoader(APKDataset(X_train), batch_size=batch_size, shuffle=True) 
val_loader = DataLoader(APKDataset(X_val), batch_size=batch_size, shuffle=False) 
 
# Training the model 
for epoch in range(num_epochs): 
    model.train() 
    total_loss = 0 
 
    # Training phase 
    for batch_data in train_loader: 
        batch_data = batch_data.to(device) 
        optimizer.zero_grad() 
        output = model(batch_data) 
        loss = criterion(output, batch_data) 
        loss.backward() 
        optimizer.step() 
        total_loss += loss.item() 
 
    avg_train_loss = total_loss / len(train_loader) 
    train_losses.append(avg_train_loss) 
 
    # Validation phase 
    model.eval() 
    total_val_loss = 0 
    with torch.no_grad(): 
        for batch_data in val_loader: 
            batch_data = batch_data.to(device) 
            output = model(batch_data) 
            val_loss = criterion(output, batch_data) 
            total_val_loss += val_loss.item() 
 
    
    avg_val_loss = total_val_loss / len(val_loader) 
    validation_losses.append(avg_val_loss) 
 
    # Print losses and learning rate 
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: 
{avg_val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.5f}") 
 
    # Step the learning rate scheduler 
    scheduler.step() 
 
# Plot training and validation loss 
plt.figure(figsize=(10, 6)) 
plt.plot(range(1, num_epochs + 1), train_losses, label="Training Loss", marker='o') 
plt.plot(range(1, num_epochs + 1), validation_losses, label="Validation Loss", marker='x') 
plt.title("Training and Validation Loss over Epochs") 
plt.xlabel("Epoch") 
plt.ylabel("Loss (MSE)") 
plt.legend() 
plt.grid(True) 
plt.show() 
 
# Save trained model 
torch.save(model.state_dict(), "./models/optimized_apk_autoencoder_model.pt") 
print("Autoencoder training complete and model saved.")
